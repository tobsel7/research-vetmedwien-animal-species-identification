{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e864ae4-ad4a-451a-b17e-5444f28fdb6b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset\"\n",
    "subtitle: \"Testing the Pre-trained Inception-ResNet-v2 Computer Vision Model on a Google Image Dataset\"\n",
    "keywords: [\"machine learning\", \"reproducibility\", \"camera tramp\", \"pre-trained model\", \"animal species classification\", \"computer vision\", \"neural networks\", \"cnn\", \"resnet\", \"tensorflow\", \"wildlife monitoring\"]\n",
    "exports: \n",
    "  - format: pdf\n",
    "    template: arxiv_two_column\n",
    "    output: document/reproducibility-experiment-animal-identification-carl.pdf\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01c608-842d-4354-a254-c21ad9dfab3a",
   "metadata": {},
   "source": [
    "+++ {\"part\": \"abstract\"}\n",
    "This experiment reproduces the results of the paper Automated detection of European wild mammal species in camera trap images with an existing and pre-trained computer vision model {cite}`doi:10.1007/s10344-020-01404-y`, \n",
    "which tests the pretrained Google Inception-ResNet-v2 model for animal species identification.\n",
    "We describe the required software, image loading processes, and model outputs. Furthermore, we calculate the prediction accuracies for each present species and the whole dataset and compare them to the metrics from the original paper. The observed total prediction accuracy of 62% comes close to the reported 71% by Carl et al. The large difference in per-class accuracy, ranging from 0% to 100%, can also be observed in our experiment. Like Carl et al., we recommend the use of the pretrained Inception-ResNet-v2 model for simple animal species identification tasks, emphasizing the need to refit the model to the species relevant for the specific use case if high prediction accuracies and consistency are desired.\n",
    "+++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37d481-9220-4329-9229-6f674b5a0e31",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "While biodiversity is decreasing at a rapid pace, the rise of specific species, be they invasive or predatory, concerns societies around the world. As a consequence, researchers and conservationists are interested in continuously monitoring wildlife populations in terms of their geographical distribution, size, and behavior. Researchers successfully deploy camera traps that can take photographs of passing animals without disturbing them {cite}`trolliet2014camera`. The photos are typically manually collected from the traps and annotated with the name of the species present in the image {cite}`10.1145/3615893.3628760`. This experiment tests one popular software for eliminating the need for manual annotation of images: deep convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3e352-9555-405e-a75a-857478b0360a",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4dde4-7acb-4884-b968-a4f1640595b1",
   "metadata": {},
   "source": [
    "We decide to reimplement the Python code for the experiment from scratch because all the necessary components (data {cite}`banerjee2024animal`, model {cite}`doi:10.48550/arXiv.1602.07261`, and metrics {cite}`scikit-learn`) can be taken from stable public sources. To maximize the readability and reproducibility of the experiment, a minimal setup was chosen, defining all necessary code, data, and requirements in one GitHub project.\n",
    "State-of-the-art Python packages are chosen, installed, and imported. The exact versions are shown in {numref}`table-requirements`. The Jupyter notebook is run locally on a Thinkpad T14 with an AMD Ryzen 5 PRO 5650U processor, 16 GB of memory, and Linux Mint 22.1 installed. No GPU was used, but it can be expected that the results would not change if one were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d66bb14-108a-4841-861a-b42ba643461c",
   "metadata": {
    "label": "pip-install",
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib==1.0.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: Pillow==11.3.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: numpy==2.1.3 in /home/tobsel/jupyter/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: pandas==2.3.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.3.1)\n",
      "Requirement already satisfied: tensorflow==2.19.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.19.0)\n",
      "Requirement already satisfied: scikit-learn==1.7.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tobsel/jupyter/lib/python3.12/site-packages (from pandas==2.3.1->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from pandas==2.3.1->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tobsel/jupyter/lib/python3.12/site-packages (from pandas==2.3.1->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (3.11.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorflow==2.19.0->-r requirements.txt (line 5)) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from scikit-learn==1.7.1->-r requirements.txt (line 6)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from scikit-learn==1.7.1->-r requirements.txt (line 6)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from scikit-learn==1.7.1->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tobsel/jupyter/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tobsel/jupyter/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tobsel/jupyter/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/tobsel/jupyter/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (14.1.0)\n",
      "Requirement already satisfied: namex in /home/tobsel/jupyter/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/tobsel/jupyter/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/tobsel/jupyter/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/tobsel/jupyter/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 5)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!python3.12 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924ea3ec-3bbb-4d52-abc8-3e0bb525a32e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 16:11:50.981266: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-13 16:11:50.984933: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-13 16:11:50.993174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757772711.006309   57140 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757772711.009993   57140 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757772711.021358   57140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757772711.021376   57140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757772711.021377   57140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757772711.021379   57140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-13 16:11:51.025224: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# image & data processing\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import decode_predictions\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceedcb57-4c22-4606-976c-08bb4749df40",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Dependency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d907ab-b266-40bf-a018-0fa32fc5c01e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "package_versions = {}\n",
    "with open(\"requirements.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"==\" in line:\n",
    "            pkg, ver = line.split(\"==\")\n",
    "            package_versions[pkg] = ver\n",
    "        else:\n",
    "            package_versions[line] = \"unknown\"\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"package\": package_versions.keys(),\n",
    "    \"version\": package_versions.values()\n",
    "}).to_markdown(\"figure/requirements.md\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61db45b-40f7-4406-b31d-308bcefef014",
   "metadata": {},
   "source": [
    "```{table} \n",
    ":name: table-requirements\n",
    ":align: center\n",
    "\n",
    "Runtime dependencies\n",
    "```{include} figure/requirements.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b4eec-e40d-49f3-a762-c6588b1b2cf5",
   "metadata": {},
   "source": [
    "# Model\n",
    "After setting up the Python runtime and importing the packages, the Inception-ResNet-v2 model is downloaded from the TensorFlow repository {cite}`tensorflow2015-whitepaper` and directly fit to the ImageNet dataset {cite}`doi:10.1109/CVPR.2009.5206848`. This eliminates all model design and training work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13ca2a5-58e7-465a-b892-e01b22f3a3d3",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 16:11:53.579282: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model = InceptionResNetV2(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2756fa1-5497-4169-93a1-7ff0cd856622",
   "metadata": {},
   "source": [
    "# Data\n",
    "Carl et al. provide the source for their wildlife images for their dataset {cite}`nationalpark2019schwarzwild`. This source is no longer available, requiring us to run the experiment on a different dataset. To test the generalizability of the model, we take a larger public dataset containing images of 90 different species {cite}`banerjee2024animal`. To mimic the original experiment setup, only 10 samples are used for each species, resulting in a total test sample size of 900 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c88843-7ffa-443b-9076-1c80ac52b33a",
   "metadata": {},
   "source": [
    "```{image} data/kaggle-90-different-animals/wombat/2a6c3fd292.jpg\n",
    ":align: center\n",
    ":alt: Example wildlife image (wombat)\n",
    ":width: 300px\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09bccc-0d76-40f1-ac09-90cea5145796",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/kaggle-90-different-animals\")\n",
    "GITHUB_PROJECT = \"https://github.com/tobsel7/research-vetmedwien-animal-species-identification\"\n",
    "input_shape = model.input_shape[1:3] # the required image dimensions (299, 299)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e7d79-7fb5-42b9-b80b-de313d1b4c5e",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We load the images respecting all three color channels (RGB), resize them to 299 by 299 pixels, and convert them into a 1-dimensional vector. The color intensities are scaled to be floating-point numbers from 0 to 1. This is the minimal preprocessing required to fit the required input size of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e40140f1-0592-413b-a60c-9aa9403ca6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, target_size):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    image = image.resize(target_size)\n",
    "    return np.array(image) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd76cbc-623f-434e-bcf1-fa3801b4d712",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "wildlife_image_paths = []\n",
    "\n",
    "animal_species = sorted([d.name for d in DATA_PATH.iterdir() if d.is_dir()])\n",
    "\n",
    "for species_name in animal_species:\n",
    "    animal_image_folder = DATA_PATH / species_name # every species has its image folder\n",
    "    for image_path in animal_image_folder.glob(\"*.jpg\"):\n",
    "        wildlife_image_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1c34b-5950-4d78-bbec-a6f9a540f6cd",
   "metadata": {},
   "source": [
    "Then we construct the testing dataset by stacking all normalized image vectors and using the folder names as the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c43f760-8384-41d0-9138-501329a5d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_images = [load_image(p, input_shape) \n",
    "                 for p in wildlife_image_paths]\n",
    "animal_species = [p.parent.name \n",
    "                  for p in wildlife_image_paths]\n",
    "\n",
    "X_test = np.stack(animal_images, axis=0)\n",
    "y_true = animal_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03016731-0ae6-439a-940c-16ffd6f7afa4",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 299, 299, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c61ad-f992-446b-9b60-b72c2e764889",
   "metadata": {},
   "source": [
    "# Test\n",
    "The model yields a probability for each of the 1000 classes. The classes represent 1000 different classes taken from the ImageNet database. For this experiment, we use the output from the top neuron of the final softmax layer and compare its label to the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57705620-c7a5-416b-9393-57a72bf9ad80",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 16:12:08.879206: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 965530800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 4s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = [pred[0][1] # take output label\n",
    "          for pred \n",
    "          in decode_predictions(y_pred, top=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2dc938-cbe0-42c5-937c-b2e2db96b132",
   "metadata": {},
   "source": [
    "When looking at the results, it becomes apparent that the model yields usable results. Almost all inference outputs are animal species somehow related to the one present in the image. This already shows that the InceptionResNetV2 is generalizable to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0b2297-2127-4b8f-bdd4-e02299daf908",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "species_recognition_result = pd.DataFrame({\n",
    "    \"y_true\": y_true,\n",
    "    \"y_pred\": y_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56b09c22-5c7b-4126-8283-d32a367d547a",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "species_recognition_result.groupby(\"y_true\", as_index=False).first()[[\"y_pred\", \"y_true\"]][:10].to_markdown(\"figure/species_recognition_result_raw_short.md\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353ed44-0309-4adc-87bf-f8845d10f55d",
   "metadata": {},
   "source": [
    "```{table} \n",
    ":name: species_recognition_result_raw_short\n",
    ":align: center\n",
    "\n",
    "Subset of Inception-ResNet-v2 raw predictions\n",
    "```{include} figure/species_recognition_result_raw_short.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8e370-e4da-4ce2-9674-8b8379824f6a",
   "metadata": {},
   "source": [
    "## Label Mapping\n",
    "The main issue with this experiment is the set of classes known to the model, which do not match the dataset used for testing. This is not unique to this dataset, but it is very likely to happen in any kind of realistic setup. We manually define a mapping table to relate the model output label to the labels from the dataset. \n",
    "\n",
    "The mapping rules are defined manually, following a best-effort approach respecting the Linnean system of taxonomy, and we acknowledge some of its shortcomings:\n",
    "\n",
    "- Some semantic information is lost as species are sometimes mapped to their families. (e.g., all bear species are mapped to bear)\n",
    "- The dataset contains images of species that are not directly related to any class from the model. (e.g., all bats, deers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f512d620-fdaa-42eb-a570-56e011303871",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "imagenet_to_kaggle = {\n",
    "    \"gazelle\": \"antelope\",\n",
    "    \"impala\": \"antelope\",\n",
    "    \"American_black_bear\": \"bear\",\n",
    "    \"brown_bear\": \"bear\",\n",
    "    \"ground_beetle\": \"beetle\",\n",
    "    \"leaf_beetle\": \"beetle\",\n",
    "    \"rhinoceros_beetle\": \"beetle\",\n",
    "    \"dung_beetle\": \"beetle\",\n",
    "    \"wild_boar\": \"boar\",\n",
    "    \"ringlet\": \"butterfly\",\n",
    "    \"monarch\": \"butterfly\",\n",
    "    \"sulphur_butterfly\": \"butterfly\",\n",
    "    \"lycaenid\": \"butterfly\",\n",
    "    \"Egyptian_cat\": \"cat\",\n",
    "    \"tabby\": \"cat\",\n",
    "    \"Siamese_cat\": \"cat\",\n",
    "    \"Persian_cat\": \"cat\",\n",
    "    \"lynx\": \"cat\",\n",
    "    \"water_buffalo\": \"cow\",\n",
    "    \"Dungeness_crab\": \"crab\",\n",
    "    \"red_deer\": \"deer\",\n",
    "    \"elk\": \"deer\",\n",
    "    \"Labrador_retriever\": \"dog\",\n",
    "    \"basset\": \"dog\",\n",
    "    \"Border_collie\": \"dog\",\n",
    "    \"Chihuahua\": \"dog\",\n",
    "    \"Bouvier_des_Flandres\": \"dog\",\n",
    "    \"Brittany_spaniel\": \"dog\",\n",
    "    \"English_setter\": \"dog\",\n",
    "    \"Greater_Swiss_Mountain_dog\": \"dog\",\n",
    "    \"Ibizan_hound\": \"dog\",\n",
    "    \"Mexican_hairless\": \"dog\",\n",
    "    \"tox_terrier\": \"dog\",\n",
    "    \"Pekinese\": \"dog\",\n",
    "    \"Pomeranian\": \"dog\", \n",
    "    \"golden_retriever\": \"dog\",\n",
    "    \"pug\": \"dog\",\n",
    "    \"ass\": \"donkey\",\n",
    "    \"mallard\": \"duck\",\n",
    "    \"bald_eagle\": \"eagle\",\n",
    "    \"golden_eagle\": \"eagle\",\n",
    "    \"African_elephant\": \"elephant\",\n",
    "    \"Indian_elephant\": \"elephant\",\n",
    "    \"Arctic_fox\": \"fox\",\n",
    "    \"red_fox\": \"fox\",\n",
    "    \"grey_fox\": \"fox\",\n",
    "    \"kit_fox\": \"fox\",\n",
    "    \"ibex\": \"goat\",\n",
    "    \"mountain_goat\": \"goat\",\n",
    "    \"Arabian_horse\": \"horse\",\n",
    "    \"Appaloosa\": \"horse\",\n",
    "    \"wallaby\": \"kangaroo\",\n",
    "    \"agama\": \"lizard\",\n",
    "    \"alligator_lizard\": \"lizard\",\n",
    "    \"banded_gecko\": \"lizard\",\n",
    "    \"Komodo_dragon\": \"lizard\",\n",
    "    \"whiptail\": \"lizard\", \n",
    "    \"American_lobster\": \"lobster\",\n",
    "    \"spiny_lobster\": \"lobster\",\n",
    "    \"house_mouse\": \"mouse\",\n",
    "    \"fiddler_crab\": \"crab\",\n",
    "    \"rock_crab\": \"crab\",\n",
    "    \"king_crab\": \"crab\",\n",
    "    \"magpie\": \"crow\",\n",
    "    \"jay\": \"crow\",\n",
    "    \"drake\": \"duck\",\n",
    "    \"tusker\": \"elephant\",\n",
    "    \"great_grey_owl\": \"owl\",\n",
    "    \"giant_panda\": \"panda\",\n",
    "    \"African_grey\": \"parrot\",\n",
    "    \"macaw\": \"parrot\",\n",
    "    \"sulphur-crested_cockatoo\": \"parrot\",\n",
    "    \"pelican\": \"pelecaniformes\",\n",
    "    \"black_stork\": \"pelecaniformes\",\n",
    "    \"king_penguin\": \"penguin\",\n",
    "    \"hog\": \"pig\",\n",
    "    \"red-backed_sandpiper\": \"sandpiper\",\n",
    "    \"redshank\": \"sandpiper\",\n",
    "    \"dowitcher\": \"sandpiper\",\n",
    "    \"great_white_shark\": \"shark\",\n",
    "    \"hammerhead\": \"shark\",\n",
    "    \"tiger_shark\": \"shark\",\n",
    "    \"horned_viper\": \"snake\",\n",
    "    \"ram\": \"sheep\",\n",
    "    \"bighorn\": \"sheep\",\n",
    "    \"vine_snake\": \"snake\",\n",
    "    \"king_snake\": \"snake\",\n",
    "    \"night_snake\": \"snake\",\n",
    "    \"Indian_cobra\": \"snake\",\n",
    "    \"ringneck_snake\": \"snake\",\n",
    "    \"rock_python\": \"snake\",\n",
    "    \"thunder_snake\": \"snake\",\n",
    "    \"fox_squirrel\": \"squirrel\",\n",
    "    \"black_swan\": \"swan\",\n",
    "    \"box_turtle\": \"turtle\",\n",
    "    \"loggerhead\": \"turtle\",\n",
    "    \"leatherback_turtle\": \"turtle\",\n",
    "    \"mud_turtle\": \"turtle\",\n",
    "    \"grey_whale\": \"whale\",\n",
    "    \"killer_whale\": \"whale\",\n",
    "    \"timber_wolf\": \"wolf\",\n",
    "    \"white_wolf\": \"wolf\"\n",
    "}\n",
    "\n",
    "y_pred_mapped = species_recognition_result[\"y_pred\"].map(\n",
    "    lambda l: imagenet_to_kaggle.get(l, l) # safe map accessor (labels that are not present in the dict keys, remain unchanged)\n",
    ")\n",
    "species_recognition_result[\"y_pred_mapped\"] = y_pred_mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e219383-81b0-4e46-ab35-b39caca164ff",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Imagenet Label Mapping Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ff4fc51-db9e-4471-82ce-5d57cc0eb868",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"ImageNet label\": imagenet_to_kaggle.keys(),\n",
    "    \"dataset label\": imagenet_to_kaggle.values()\n",
    "}).groupby(\"dataset label\")[\"ImageNet label\"].apply(\n",
    "    lambda labels: \", \".join(labels)\n",
    ").reset_index()[[\"ImageNet label\", \"dataset label\"]][:10].to_markdown(\"figure/imagenet_label_mapping.md\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497400d7-48af-4391-b5cb-2be51d2700b1",
   "metadata": {},
   "source": [
    "```{table} \n",
    ":name: imagenet_label_mapping\n",
    ":align: center\n",
    "\n",
    "Mapping rules between ImageNet classes and test data classes\n",
    "```{include} figure/imagenet_label_mapping.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1bb2ff-e885-4ca9-8095-ceff8fe1a675",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Carl et al. provide two kinds of performance metrics: overall model accuracy and the accuracy for each species. By grouping the samples by true species, it is straightforward to calculate metrics that are semantically equivalent and therefore allow us to use them for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d64d76-4955-4fae-a9ff-2c5a6dd591a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_true, y_pred_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46fe9de-4ba5-4fb9-a165-66dc49cc2d8b",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a917030d-7e9c-499f-bcbf-fcd60b4a9ba2",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "group_accuracy = species_recognition_result.assign(\n",
    "    accuracy = species_recognition_result[\"y_true\"] == species_recognition_result[\"y_pred_mapped\"]\n",
    ").groupby(\"y_true\")[\"accuracy\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e716dd3-c719-4626-bdf1-6ccbf6f80bb9",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_true\n",
       "bison         1.0\n",
       "bear          1.0\n",
       "boar          1.0\n",
       "crab          1.0\n",
       "elephant      1.0\n",
       "             ... \n",
       "reindeer      0.0\n",
       "squid         0.0\n",
       "sparrow       0.0\n",
       "turkey        0.0\n",
       "woodpecker    0.0\n",
       "Name: accuracy, Length: 90, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66631238-cecf-4ed5-9f4f-fcecfc570d1f",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Group Accuracy Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa4b1d98-c1f2-4dde-bf3f-caf6b167369e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "grouped_species = group_accuracy.reset_index()\n",
    "grouped_species.columns = [\"species\", \"accuracy\"]\n",
    "\n",
    "summary_grouped = (\n",
    "    grouped_species.groupby(\"accuracy\")[\"species\"]\n",
    "    .apply(lambda x: \", \".join(x))\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"accuracy\", ascending=False)\n",
    ")[[\"species\", \"accuracy\"]]\n",
    "\n",
    "md_table = pd.concat([\n",
    "    summary_grouped,\n",
    "    pd.DataFrame([[\"...\", \"...\"]], columns=[\"species\", \"accuracy\"]),\n",
    "    pd.DataFrame([[\"TOTAL\", accuracy]], columns=[\"species\", \"accuracy\"])\n",
    "], ignore_index=True)\n",
    "\n",
    "md_table.to_markdown(\"figure/species_recognition_result_summary.md\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f40159-ba3e-4f61-b036-ee8d976dbd79",
   "metadata": {},
   "source": [
    "```{table} \n",
    ":name: species_recognition_result_summary\n",
    ":align: center\n",
    "\n",
    "Prediction accuracy per species and the total accuracy\n",
    "```{include} figure/species_recognition_result_summary.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0bc3b2-7243-4663-a109-44c1ceab5ca9",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "TODO: Carl et al. did a good job and their results are valid. It is important to emphasize the fact that the model can only predict a limited number of species. This makes the model almost certainly unsuitable for use in real use cases. The neural network architecture is powerful and very accurate in predicting classes that it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1657b70-9021-4849-9e20-ad8d18353e74",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "TODO: Find paper that demonstrates transfer lerning using the inceptionresnet model and explain how great accuracy can be achived also for other species. Also mention that there are still few projects that deploy camera traps with such models and really have all of this automated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4cafb-91a2-42d0-84cd-12f62816805b",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "# Experiment Artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b1f8c9-6a23-4f9f-b603-42008a0a5089",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Inference Output Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16fc0cdf-5032-40d7-9f29-7c213f19e781",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "github_animal_image_paths = [\n",
    "    f\"[{p.parent.name}]({GITHUB_PROJECT}/raw/main/{p})\"\n",
    "    for p in wildlife_image_paths\n",
    "]\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"image link\": github_animal_image_paths,\n",
    "    \"truth\": species_recognition_result[\"y_true\"],\n",
    "    \"mapped prediction\": species_recognition_result[\"y_pred_mapped\"],\n",
    "    \"model prediction\": species_recognition_result[\"y_pred\"]\n",
    "}).to_csv(\"figure/model_prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
